# ğŸŒ Open World Risk - Risk Assessment Pipeline

> A complete machine learning pipeline for assessing risk in open-world environments using RGB-D vision and Bayesian inference.

This system processes RGB-D video data to generate spatial risk assessments by combining **prior knowledge** (common-sense understanding) with **likelihood observations** (real-world data) through Bayesian posterior computation.

---

## ğŸ“Š Pipeline Overview

The system follows a four-stage Bayesian inference pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ“¹ Video   â”‚ â†’  â”‚  ğŸ§  Prior    â”‚ â†’  â”‚  ğŸ“Š Like-    â”‚ â†’  â”‚  ğŸ”— Post-    â”‚
â”‚  Processing  â”‚    â”‚   Training   â”‚    â”‚   lihood     â”‚    â”‚   erior      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                    â†“                    â†“                    â†“
video_processing/   common_sense_priors/  data_deriv_likelihood/  posterior/
```

**Bayesian Framework:**  
`P(risk | observations) âˆ P(observations | risk) Ã— P(risk)`

## ğŸ—ï¸ Pipeline Components

### ğŸ“¹ **1. Video Processing** (`video_processing/`)

Processes RGB-D video data to extract object-level features and risk information.

**Key Scripts:**
- `process_video.py` - Main processing script with object detection
- `process_video_utils.py` - Utility functions
- `images_to_video.py` - Frame-to-video conversion

> ğŸ“¥ **Input:** RGB-D video data (.npz files with RGB, depth, pose)  
> ğŸ“¤ **Output:** Object masks, features, manipulation scores per frame

---

### ğŸ§  **2. Prior Training** (`common_sense_priors/`)

Trains models to encode common-sense knowledge about object risk.

**Key Scripts:**
- `train_prior.py` - Prior model training
- `model_prior.py` - Model architecture
- `dataset_prior.py` - Dataset handling
- `batch_prior_over_folders.py` - Batch processing

> ğŸ“¥ **Input:** Synthetic risk data, object labels  
> ğŸ“¤ **Output:** Trained prior models with risk knowledge

---

### ğŸ“Š **3. Likelihood Training** (`data_deriv_likelihood/`)

Extracts risk likelihood from real video observations.

**Key Scripts:**
- `inference_riskfield_video.py` - Main inference script
- `inference_riskfield_video_lite.py` - Lightweight version
- `build_lookup_likelihood.py` - Lookup table builder
- `likelihood_from_single_frame.py` - Single frame processor

> ğŸ“¥ **Input:** Processed video from `video_processing/`  
> ğŸ“¤ **Output:** Risk likelihood maps per frame

---

### ğŸ”— **4. Posterior Combination** (`posterior/`)

Combines prior knowledge with likelihood observations for final risk assessment.

**Key Scripts:**
- `combine_likelihood_and_prior.py` - Basic combination (single object)
- `combine_likelihood_and_prior_tree.py` - Advanced with depth modulation
- `combine_likelihood_and_prior_tree_.py` - Multi-object without depth
- `posterior_to_ply.py` - 3D point cloud generator

> ğŸ“¥ **Input:** Prior risk maps + likelihood risk maps  
> ğŸ“¤ **Output:** Final posterior risk assessments (2D + 3D)

## ğŸš€ Getting Started

### ğŸ“¦ Step 1: Pull Docker Images

```bash
docker pull bayesianrisk/open_world_risk_main:latest &
docker pull bayesianrisk/open_world_risk_hoist_former:latest &
```

### ğŸŒ Step 2: Create Docker Network

```bash
docker network create open_world_risk_net
```

### ğŸ–¥ï¸ Step 3: Launch Containers

**Start Hoist Former Container (Server)**
```bash 
docker run --rm \
     --runtime=nvidia --gpus all \
     --network open_world_risk_net \
     -p 5002:5000 \
     -v $(pwd):/workspace -w /workspace \
     -e PYTHONWARNINGS=ignore::FutureWarning \
     -e PYTHONPATH=/workspace/open_world_risk/hoist_former/Mask2Former \
     --name hoist_former \
     bayesianrisk/open_world_risk_hoist_former:latest \
     /bin/bash -c "source /home/ubuntu/miniconda3/etc/profile.d/conda.sh && \
                   conda activate hoist2 && \
                   cd /workspace/open_world_risk/hoist_former/Mask2Former && \
                   pip install -qq -r requirements.txt && \
                   cd mask2former/modeling/pixel_decoder/ops && \
                   sh make.sh > /dev/null 2>&1 && \
                   pip install -qq setuptools==59.5.0 && \
                   cd /workspace && \
                   python -c 'import mask2former; print(\"ğŸš€ Mask2Former ready!\")' && \
                   python open_world_risk/hoist_former/api/endpoints.py"
```

**Start Main Container (Interactive)**
```bash 
docker run -it --rm \
     --runtime=nvidia --gpus all \
     --shm-size=8gb \
     --network open_world_risk_net \
     -p 5001:5000 \
     -v $(pwd):/workspace -w /workspace \
     -e PYTHONWARNINGS=ignore::UserWarning,ignore::FutureWarning \
     --name main \
     bayesianrisk/open_world_risk_main:latest \
     bash
```

**Activate Python Environment (inside main container)**
```bash
conda activate feature_splatting2
```

---

## ğŸ”„ Running the Complete Pipeline

Execute the following steps in order (all commands run inside the main container):
0. **Download Docker Images and Run Containers
***Pull images from Docker Hub***
```bash
docker pull bayesianrisk/open_world_risk_main:latest &
docker pull bayesianrisk/open_world_risk_hoist_former:latest &
```
***Create Network for Containers to Communicate***
```bash
docker network create open_world_risk_net
```
***Run hoist_former container as server***
```bash 
docker run --rm \
     --runtime=nvidia --gpus all \
     --network open_world_risk_net \
     -p 5002:5000 \
     -v $(pwd):/workspace -w /workspace \
     -e PYTHONWARNINGS=ignore::FutureWarning \
     -e PYTHONPATH=/workspace/open_world_risk/hoist_former/Mask2Former \
     --name hoist_former \
     doku88/open_world_risk_hoist_former:latest \
     /bin/bash -c "source /home/ubuntu/miniconda3/etc/profile.d/conda.sh && conda activate hoist2 && cd /workspace/open_world_risk/hoist_former/Mask2Former && pip install -qq -r requirements.txt && cd mask2former/modeling/pixel_decoder/ops && sh make.sh > /dev/null 2>&1 && pip install -qq setuptools==59.5.0 && cd /workspace && python -c 'import mask2former; print(\"ğŸš€ğŸš€ğŸš€ Mask2Former import successful! ğŸš€ğŸš€ğŸš€\")' && python open_world_risk/hoist_former/api/endpoints.py"
```
***Run main container interactively (all subsequent commands will be executed in the main container)***
```bash 
docker run -it --rm \
     --runtime=nvidia --gpus all \
     --shm-size=8gb \
     --network open_world_risk_net \
     -p 5001:5000 \
     -v $(pwd):/workspace -w /workspace \
     -e PYTHONWARNINGS=ignore::UserWarning,ignore::FutureWarning \
     --name main \
     doku88/open_world_risk_main:latest \
     bash
```
***In main container activate the python environment***
```bash
conda activate feature_splatting2
```

### ğŸ“¹ **Step 1: Process Video Data**
Extract object features and risk information from RGB-D video:
```bash
python open_world_risk/video_processing/process_video.py
```

### ğŸ§  **Step 2: Train Prior Models**
Train models on common-sense risk knowledge:
```bash
python open_world_risk/common_sense_priors/train_prior.py
```

### ğŸ“Š **Step 3: Generate Likelihood from Real Data**
Extract risk likelihood from processed video:
```bash
python open_world_risk/data_deriv_likelihood/inference_riskfield_video.py
```

### ğŸ”— **Step 4: Combine Prior + Likelihood**
Compute final posterior risk assessments:
```bash
python open_world_risk/posterior/combine_likelihood_and_prior_tree.py
```

### ğŸ¨ **Step 5: Generate 3D Risk Visualization**
Convert posterior to 3D point clouds:
```bash
python open_world_risk/posterior/posterior_to_ply.py
```

---

## âš™ï¸ Configuration

Each pipeline component uses YAML configuration files for easy customization:

| Component | Config File |
|-----------|------------|
| ğŸ“¹ Video Processing | `video_processing/process_video_config.yaml` |
| ğŸ§  Prior Training | `common_sense_priors/train_prior_config.yaml` |
| ğŸ“Š Likelihood | `data_deriv_likelihood/infer_risk_field_vid_cfg.yaml` |
| ğŸ”— Posterior | `posterior/combine_tree_config.yaml` |

### ğŸ”‘ **Important: WandB Configuration**

Before running the training scripts, you **must** configure your Weights & Biases credentials:

1. **Create a WandB account** at [wandb.ai](https://wandb.ai) (if you don't have one)
2. **Update the following config files** with your WandB username:
   - `open_world_risk/train/train_config.yaml` (line 17)
   - `open_world_risk/common_sense_priors/train_prior_config.yaml` (line 17)

Replace `"userA"` with your actual WandB username:
```yaml
wandb:
  project: "open_world_risk_0"
  run_name: "rf_bezier_supervision"
  entity: "your_wandb_username"  # â† Replace "userA" with YOUR username
```

> ğŸ’¡ **Tip:** Set `entity: null` if you're not using a WandB team/organization.

---

## âœ¨ Key Features

- ğŸ¯ **Multi-object Risk Assessment** - Handles multiple objects simultaneously
- ğŸ“ **Depth-aware Processing** - Incorporates depth information for 3D risk modeling
- ğŸ¨ **3D Visualization** - Generates PLY point clouds for risk visualization
- ğŸ§© **Modular Design** - Each component can be used independently

---

## ğŸ“‹ Requirements

- ğŸ‹ Docker (to download containers with environment)
- ğŸ–¥ï¸ NVIDIA GPU (CUDA-compatible) required for training/inference  
  _(We used an RTX 4090 for our experiments.)_


